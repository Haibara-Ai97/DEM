#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Evaluation script for the concrete QA JSONL dataset (yesno, multilabel, count, grid, json).
Works with base models or LoRA adapters produced by scripts.vlm_baselines.train.

Example:
  python -m scripts.vlm_baselines.eval \
    --model_id Qwen/Qwen2.5-VL-7B-Instruct \
    --family qwen2_5_vl \
    --adapter_dir /path/to/lora_out \
    --jsonl /path/to/qa_test.jsonl \
    --split test

Author: generated by ChatGPT
"""
from __future__ import annotations

import argparse
import json
import math
import re
from collections import defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch

from dem.config_utils import load_yaml
from models.vlm_baselines import build_messages_prompt, load_model_and_processor


DEFAULT_CONFIG_PATH = Path(__file__).resolve().parents[2] / "configs" / "vlm_baselines" / "default.yaml"
DEFAULT_EVAL = {
    "bf16": False,
    "qwen_min_pixels": 0,
    "qwen_max_pixels": 0,
    "max_samples": 0,
    "max_new_tokens_yesno": 4,
    "max_new_tokens_multilabel": 24,
    "max_new_tokens_count": 8,
    "max_new_tokens_grid": 8,
    "max_new_tokens_json": 256,
}


# -------------------------
# I/O
# -------------------------
def read_jsonl(path: str, split: Optional[str]) -> List[Dict[str, Any]]:
    p = Path(path)
    assert p.exists(), f"Missing jsonl: {p}"
    out = []
    with p.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            ex = json.loads(line)
            if split is not None and ex.get("split") != split:
                continue
            out.append(ex)
    return out


# -------------------------
# Parsing helpers
# -------------------------
DEFECTS = {"crack", "spalling", "honeycomb", "hole", "exposed rebar", "seepage"}

def norm_ws(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip())

def parse_yesno(s: str) -> Optional[str]:
    s2 = norm_ws(s).lower()
    # keep only first token-ish
    if "yes" in s2[:10]:
        return "Yes"
    if "no" in s2[:10]:
        return "No"
    # sometimes model outputs "Answer: Yes"
    if "yes" in s2:
        return "Yes"
    if "no" in s2:
        return "No"
    return None

def parse_int(s: str) -> Optional[int]:
    m = re.search(r"-?\d+", s)
    if not m:
        return None
    try:
        return int(m.group(0))
    except Exception:
        return None

def parse_grid_cell(s: str) -> Optional[Tuple[int, int]]:
    # r#c#
    m = re.search(r"r\s*(\d+)\s*c\s*(\d+)", s.lower().replace(" ", ""))
    if not m:
        return None
    r = int(m.group(1))
    c = int(m.group(2))
    return r, c

def parse_multilabel(s: str) -> Optional[List[str]]:
    s2 = norm_ws(s).lower()
    if not s2:
        return None
    if "none" in s2 and all(d not in s2 for d in DEFECTS):
        return []
    # split by comma/semicolon/newline
    parts = re.split(r"[,\n;]+", s2)
    out = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        # normalize known phrases
        p = p.replace("rebar", "rebar")
        # allow small variants
        if p in DEFECTS:
            out.append(p)
        elif p.replace("_", " ") in DEFECTS:
            out.append(p.replace("_", " "))
    # if nothing matched, return None (invalid)
    return out if out else None

def parse_json_summary(s: str) -> Tuple[Optional[Dict[str, Any]], bool]:
    """
    Returns (obj, schema_ok)
    """
    try:
        obj = json.loads(s)
    except Exception:
        # try to extract first {...} block
        m = re.search(r"\{.*\}", s, re.S)
        if not m:
            return None, False
        try:
            obj = json.loads(m.group(0))
        except Exception:
            return None, False

    # schema check
    if not isinstance(obj, dict):
        return obj, False
    if "defects" not in obj or "overall_condition" not in obj:
        return obj, False
    if not isinstance(obj["defects"], list):
        return obj, False
    if not isinstance(obj["overall_condition"], str):
        return obj, False
    for d in obj["defects"]:
        if not isinstance(d, dict):
            return obj, False
        for k in ("type", "count", "severity", "primary_cell"):
            if k not in d:
                return obj, False
    return obj, True


# -------------------------
# Metrics
# -------------------------
def f1_binary(tp, fp, fn) -> float:
    denom = 2 * tp + fp + fn
    return 0.0 if denom == 0 else (2 * tp) / denom

def macro_f1_yesno(y_true: List[str], y_pred: List[str]) -> float:
    # class-wise f1 for Yes and No
    f1s = []
    for cls in ("Yes", "No"):
        tp = sum(1 for t, p in zip(y_true, y_pred) if t == cls and p == cls)
        fp = sum(1 for t, p in zip(y_true, y_pred) if t != cls and p == cls)
        fn = sum(1 for t, p in zip(y_true, y_pred) if t == cls and p != cls)
        f1s.append(f1_binary(tp, fp, fn))
    return sum(f1s) / 2.0

def balanced_acc_yesno(y_true: List[str], y_pred: List[str]) -> float:
    # average recall over two classes
    recs = []
    for cls in ("Yes", "No"):
        tp = sum(1 for t, p in zip(y_true, y_pred) if t == cls and p == cls)
        fn = sum(1 for t, p in zip(y_true, y_pred) if t == cls and p != cls)
        rec = 0.0 if (tp + fn) == 0 else tp / (tp + fn)
        recs.append(rec)
    return sum(recs) / 2.0

def micro_f1_multilabel(trues: List[List[str]], preds: List[List[str]]) -> float:
    tp = fp = fn = 0
    for t, p in zip(trues, preds):
        ts, ps = set(t), set(p)
        tp += len(ts & ps)
        fp += len(ps - ts)
        fn += len(ts - ps)
    denom = 2 * tp + fp + fn
    return 0.0 if denom == 0 else (2 * tp) / denom

def example_f1_multilabel(trues: List[List[str]], preds: List[List[str]]) -> float:
    f1s = []
    for t, p in zip(trues, preds):
        ts, ps = set(t), set(p)
        tp = len(ts & ps)
        fp = len(ps - ts)
        fn = len(ts - ps)
        f1s.append(f1_binary(tp, fp, fn))
    return sum(f1s) / len(f1s) if f1s else 0.0

def mae_rmse(vals: List[int]) -> Tuple[float, float]:
    if not vals:
        return 0.0, 0.0
    mae = sum(abs(v) for v in vals) / len(vals)
    rmse = math.sqrt(sum(v * v for v in vals) / len(vals))
    return mae, rmse

def grid_l1(a: Tuple[int, int], b: Tuple[int, int]) -> int:
    return abs(a[0] - b[0]) + abs(a[1] - b[1])

def f1_set(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    tp = len(sa & sb)
    fp = len(sb - sa)
    fn = len(sa - sb)
    return f1_binary(tp, fp, fn)


# -------------------------
# Encoding & generation
# -------------------------
@dataclass
class Encoder:
    family: str
    processor: Any
    tokenizer: Any

    def __post_init__(self):
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            from qwen_vl_utils import process_vision_info  # type: ignore
            self._qwen_process_vision_info = process_vision_info
        else:
            self._qwen_process_vision_info = None
        from PIL import Image
        self._Image = Image

    def load_image(self, path: str):
        return self._Image.open(path).convert("RGB")

    def build_inputs(self, sample: Dict[str, Any]):
        msgs, _ = build_messages_prompt(sample, self.family)
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            text = self.processor.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            image_inputs, video_inputs = self._qwen_process_vision_info(msgs)
            # Do not pass empty videos lists (no-video dataset).
            if video_inputs is not None and isinstance(video_inputs, (list, tuple)) and len(video_inputs) == 0:
                video_inputs = None
            if video_inputs is None:
                inputs = self.processor(text=[text], images=image_inputs, padding=True, return_tensors="pt")
            else:
                inputs = self.processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors="pt")
            return inputs, inputs["input_ids"].shape[1]
        if self.family == "phi3v":
            prompt = self.tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
            img = self.load_image(sample["image_path"])
            inputs = self.processor(prompt, [img], return_tensors="pt").to("cuda:0")
            return inputs, inputs["input_ids"].shape[1]
        else:
            prompt = self.processor.apply_chat_template(msgs, add_generation_prompt=True)
            img = self.load_image(sample["image_path"])
            inputs = self.processor(images=img, text=prompt, return_tensors="pt").to("cuda:0")
            return inputs, inputs["input_ids"].shape[1]


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", type=str, default=str(DEFAULT_CONFIG_PATH))
    ap.add_argument("--model_key", type=str, default="")
    ap.add_argument("--model_id", type=str)
    ap.add_argument("--family", type=str,
                    choices=["qwen2_vl", "qwen2_5_vl", "llava_1_5", "idefics2", "phi3v"])
    ap.add_argument("--adapter_dir", type=str, default="")
    ap.add_argument("--jsonl", type=str, required=True)
    ap.add_argument("--split", type=str, default="test", choices=["train", "valid", "test"])

    ap.add_argument("--bf16", action="store_true", default=None)

    # Qwen-VL visual token budget (optional, to control visual tokens at eval time)
    ap.add_argument("--qwen_min_pixels", type=int)
    ap.add_argument("--qwen_max_pixels", type=int)
    ap.add_argument("--max_samples", type=int)

    ap.add_argument("--max_new_tokens_yesno", type=int)
    ap.add_argument("--max_new_tokens_multilabel", type=int)
    ap.add_argument("--max_new_tokens_count", type=int)
    ap.add_argument("--max_new_tokens_grid", type=int)
    ap.add_argument("--max_new_tokens_json", type=int)

    args = ap.parse_args()

    cfg = {}
    if args.config and Path(args.config).exists():
        cfg = load_yaml(args.config)

    model_cfg = {}
    if args.model_key:
        model_cfg = cfg.get("models", {}).get(args.model_key, {})
    elif "model" in cfg:
        model_cfg = cfg.get("model", {})

    eval_cfg = cfg.get("evaluation", {})

    def _resolve(value, key):
        if value is not None:
            return value
        if key in eval_cfg:
            return eval_cfg[key]
        return DEFAULT_EVAL[key]

    args.model_id = args.model_id or model_cfg.get("model_id")
    args.family = args.family or model_cfg.get("family")
    if not args.model_id or not args.family:
        raise ValueError("model_id/family must be provided via CLI or config.")

    args.bf16 = _resolve(args.bf16, "bf16")
    args.qwen_min_pixels = _resolve(args.qwen_min_pixels, "qwen_min_pixels")
    args.qwen_max_pixels = _resolve(args.qwen_max_pixels, "qwen_max_pixels")
    args.max_samples = _resolve(args.max_samples, "max_samples")
    args.max_new_tokens_yesno = _resolve(args.max_new_tokens_yesno, "max_new_tokens_yesno")
    args.max_new_tokens_multilabel = _resolve(args.max_new_tokens_multilabel, "max_new_tokens_multilabel")
    args.max_new_tokens_count = _resolve(args.max_new_tokens_count, "max_new_tokens_count")
    args.max_new_tokens_grid = _resolve(args.max_new_tokens_grid, "max_new_tokens_grid")
    args.max_new_tokens_json = _resolve(args.max_new_tokens_json, "max_new_tokens_json")

    data = read_jsonl(args.jsonl, split=args.split)
    if args.max_samples:
        data = data[: args.max_samples]

    model, processor, tokenizer = load_model_and_processor(args.model_id, args.family, args.adapter_dir, args.bf16, args.qwen_min_pixels, args.qwen_max_pixels)
    enc = Encoder(args.family, processor, tokenizer)

    # accumulators
    yesno_true, yesno_pred, yesno_defect, yesno_valid = [], [], [], []
    ml_true, ml_pred, ml_valid = [], [], []
    cnt_errs, cnt_exact, cnt_within1, cnt_valid = [], 0, 0, []
    grid_l1s, grid_exact, grid_within1, grid_valid = [], 0, 0, []
    json_parse_ok, json_schema_ok = [], []
    json_overall_acc = []
    json_type_f1s, json_count_maes, json_sev_accs, json_cell_accs = [], [], [], []

    for ex in data:
        task = ex["task"]
        inputs, in_len = enc.build_inputs(ex)
        # move to cuda
        if args.family in ("qwen2_vl", "qwen2_5_vl"):
            inputs = {k: v.to("cuda") if hasattr(v, "to") else v for k, v in inputs.items()}
        # pick max_new_tokens
        max_new = {
            "yesno": args.max_new_tokens_yesno,
            "multilabel": args.max_new_tokens_multilabel,
            "count": args.max_new_tokens_count,
            "grid": args.max_new_tokens_grid,
            "json": args.max_new_tokens_json,
        }[task]

        with torch.no_grad():
            out = model.generate(**inputs, max_new_tokens=max_new, do_sample=False)

        # trim prompt
        out_ids = out[:, in_len:]
        pred_raw = processor.batch_decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
        pred_raw = pred_raw.strip()

        if task == "yesno":
            gt = ex["assistant"].strip()
            pr = parse_yesno(pred_raw)
            yesno_true.append(gt)
            yesno_pred.append(pr if pr is not None else "INVALID")
            yesno_defect.append(ex["meta"]["defect"])
            yesno_valid.append(pr is not None)

        elif task == "multilabel":
            gt = ex["meta"]["present"]
            pr = parse_multilabel(pred_raw)
            ml_true.append(gt)
            ml_pred.append(pr if pr is not None else [])
            ml_valid.append(pr is not None)

        elif task == "count":
            gt = int(ex["meta"]["count"])
            pr = parse_int(pred_raw)
            cnt_valid.append(pr is not None)
            if pr is None:
                pr = 0
            err = pr - gt
            cnt_errs.append(err)
            cnt_exact += int(err == 0)
            cnt_within1 += int(abs(err) <= 1)

        elif task == "grid":
            gt_cell = ex["meta"]["primary_cell"]
            gt = parse_grid_cell(gt_cell)
            pr = parse_grid_cell(pred_raw)
            grid_valid.append(pr is not None)
            if gt is None:
                continue
            if pr is None:
                pr = (-999, -999)
            l1 = grid_l1(gt, pr)
            grid_l1s.append(l1)
            grid_exact += int(l1 == 0)
            grid_within1 += int(l1 <= 1)

        elif task == "json":
            gt_obj = json.loads(ex["assistant"])
            pr_obj, schema_ok = parse_json_summary(pred_raw)
            json_parse_ok.append(pr_obj is not None)
            json_schema_ok.append(schema_ok)

            if pr_obj is None or not schema_ok:
                # treat as 0 for all metrics
                json_overall_acc.append(0.0)
                json_type_f1s.append(0.0)
                json_count_maes.append(0.0)
                json_sev_accs.append(0.0)
                json_cell_accs.append(0.0)
                continue

            # overall_condition acc
            json_overall_acc.append(float(pr_obj.get("overall_condition") == gt_obj.get("overall_condition")))

            # type set f1
            gt_types = [d["type"] for d in gt_obj["defects"]]
            pr_types = [d.get("type", "") for d in pr_obj["defects"]]
            json_type_f1s.append(f1_set(gt_types, pr_types))

            # per-type count mae (missing => 0)
            gt_map = {d["type"]: int(d["count"]) for d in gt_obj["defects"]}
            pr_map = {d.get("type", ""): int(d.get("count", 0)) for d in pr_obj["defects"] if isinstance(d.get("count", None), (int, float, str))}
            union = sorted(set(gt_map) | set(pr_map))
            if union:
                maes = [abs(int(pr_map.get(t, 0)) - int(gt_map.get(t, 0))) for t in union]
                json_count_maes.append(sum(maes) / len(maes))
            else:
                json_count_maes.append(0.0)

            # severity acc and cell acc on GT types (ignore extras)
            sev_ok = 0
            cell_ok = 0
            denom = max(1, len(gt_obj["defects"]))
            pr_def_by_type = {d.get("type"): d for d in pr_obj["defects"] if isinstance(d, dict)}
            for d in gt_obj["defects"]:
                t = d["type"]
                pd = pr_def_by_type.get(t)
                if pd is None:
                    continue
                sev_ok += int(str(pd.get("severity")) == str(d.get("severity")))
                cell_ok += int(str(pd.get("primary_cell")) == str(d.get("primary_cell")))
            json_sev_accs.append(sev_ok / denom)
            json_cell_accs.append(cell_ok / denom)

        else:
            raise ValueError(f"Unknown task: {task}")

    # -------------------------
    # Print results
    # -------------------------
    print("========== EVAL RESULTS ==========")

    # yesno
    if yesno_true:
        valid_rate = sum(yesno_valid) / len(yesno_valid)
        # Only evaluate on valid preds; invalids count as wrong
        y_pred_clean = [p if p in ("Yes", "No") else ("No" if t == "Yes" else "Yes") for t, p in zip(yesno_true, yesno_pred)]
        acc = sum(int(t == p) for t, p in zip(yesno_true, y_pred_clean)) / len(yesno_true)
        bacc = balanced_acc_yesno(yesno_true, y_pred_clean)
        mf1 = macro_f1_yesno(yesno_true, y_pred_clean)
        invalid = sum(1 for v in yesno_valid if not v)
        print(f"[yesno] valid_rate={valid_rate:.4f}  acc={acc:.4f}  bacc={bacc:.4f}  macro_f1={mf1:.4f}  invalid={invalid}")

        # per-defect breakdown
        by_def = defaultdict(list)
        for t, p, d, v in zip(yesno_true, y_pred_clean, yesno_defect, yesno_valid):
            by_def[d].append((t, p, v))
        print("[yesno] per-defect (top 20 by support):")
        for d, rows in sorted(by_def.items(), key=lambda x: len(x[1]), reverse=True)[:20]:
            tlist = [r[0] for r in rows]
            plist = [r[1] for r in rows]
            vlist = [r[2] for r in rows]
            d_acc = sum(int(t == p) for t, p in zip(tlist, plist)) / len(rows)
            d_bacc = balanced_acc_yesno(tlist, plist)
            d_mf1 = macro_f1_yesno(tlist, plist)
            d_valid = sum(vlist) / len(vlist)
            print(f"  - {d:14s} n={len(rows):4d}   acc={d_acc:.3f} bacc={d_bacc:.3f} macro_f1={d_mf1:.3f} valid={d_valid:.3f}")

    # multilabel
    if ml_true:
        valid_rate = sum(ml_valid) / len(ml_valid)
        invalid = len(ml_valid) - sum(ml_valid)
        mf1 = micro_f1_multilabel(ml_true, ml_pred)
        ef1 = example_f1_multilabel(ml_true, ml_pred)
        print(f"[multilabel] valid_rate={valid_rate:.4f}  micro_f1={mf1:.4f}  example_f1={ef1:.4f}  invalid={invalid}")

    # count
    if cnt_errs:
        valid_rate = sum(cnt_valid) / len(cnt_valid)
        invalid = len(cnt_valid) - sum(cnt_valid)
        exact = cnt_exact / len(cnt_errs)
        within1 = cnt_within1 / len(cnt_errs)
        mae, rmse = mae_rmse(cnt_errs)
        print(f"[count] valid_rate={valid_rate:.4f}  exact_acc={exact:.4f}  within1_acc={within1:.4f}  MAE={mae:.4f}  RMSE={rmse:.4f}  invalid={invalid}")

    # grid
    if grid_l1s:
        valid_rate = sum(grid_valid) / len(grid_valid)
        invalid = len(grid_valid) - sum(grid_valid)
        exact = grid_exact / len(grid_l1s)
        within1 = grid_within1 / len(grid_l1s)
        mean_l1 = sum(grid_l1s) / len(grid_l1s)
        print(f"[grid] valid_rate={valid_rate:.4f}  exact_acc={exact:.4f}  within1_acc={within1:.4f}  mean_L1={mean_l1:.4f}  invalid={invalid}")

    # json
    if json_parse_ok:
        parse_rate = sum(json_parse_ok) / len(json_parse_ok)
        schema_rate = sum(json_schema_ok) / len(json_schema_ok)
        overall_acc = sum(json_overall_acc) / len(json_overall_acc)
        type_f1 = sum(json_type_f1s) / len(json_type_f1s)
        count_mae = sum(json_count_maes) / len(json_count_maes)
        sev_acc = sum(json_sev_accs) / len(json_sev_accs)
        cell_acc = sum(json_cell_accs) / len(json_cell_accs)
        print(f"[json] parse_rate={parse_rate:.4f}  schema_rate={schema_rate:.4f}  overall_acc={overall_acc:.4f}  type_f1={type_f1:.4f}  count_MAE={count_mae:.4f}  severity_acc={sev_acc:.4f}  cell_acc={cell_acc:.4f}")


if __name__ == "__main__":
    main()
