#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRA SFT baseline runner for small/medium MLLMs on the concrete QA JSONL dataset.

Supported families (all are mainstream HF-hosted models):
  - qwen2_vl       : Qwen/Qwen2-VL-7B-Instruct
  - qwen2_5_vl     : Qwen/Qwen2.5-VL-7B-Instruct
  - llava_1_5      : llava-hf/llava-1.5-7b-hf
  - idefics2       : HuggingFaceM4/idefics2-8b
  - phi3v          : microsoft/Phi-3.5-vision-instruct

Dataset format: one json per line with fields produced by make_concrete_qa_jsonl_autosplit.py:
  image_path, system, user, assistant, task, split, meta

Multi-GPU: run with torchrun, e.g.
  torchrun --nproc_per_node=8 -m scripts.vlm_baselines.train ...args...

Notes:
  - For Qwen2/2.5-VL, install qwen-vl-utils and (recommended) transformers from source.
  - This script uses PEFT LoRA. Optionally supports QLoRA (--load_in_4bit).

Author: generated by ChatGPT
"""
from __future__ import annotations

import argparse
import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForVision2Seq,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    set_seed,
)

from utils.config_utils import load_yaml
from models.vlm_baselines import (
    build_messages_full,
    build_messages_prompt,
    infer_lora_targets,
    safe_auto_processor_from_pretrained,
)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


DEFAULT_CONFIG_PATH = Path(__file__).resolve().parents[2] / "configs" / "vlm_baselines" / "default.yaml"
DEFAULT_TRAINING = {
    "max_length": 512,
    "epochs": 1.0,
    "lr": 2e-4,
    "weight_decay": 0.0,
    "warmup_ratio": 0.03,
    "per_device_train_batch_size": 1,
    "per_device_eval_batch_size": 1,
    "grad_accum": 8,
    "max_grad_norm": 1.0,
    "bf16": False,
    "fp16": False,
    "load_in_4bit": False,
    "gradient_checkpointing": False,
    "lora_r": 16,
    "lora_alpha": 32,
    "lora_dropout": 0.05,
    "qwen_min_pixels": 0,
    "qwen_max_pixels": 0,
    "qwen_min_tokens": 256,
    "qwen_max_tokens": 256,
    "use_slow_processor": False,
    "seed": 42,
    "max_train_samples": 0,
    "max_eval_samples": 0,
    "logging_steps": 10,
    "save_steps": 500,
    "eval_steps": 500,
}


# ---------------------------
# Dataset
# ---------------------------
class ConcreteQADataset(Dataset):
    def __init__(self, jsonl_path: str, split: Optional[str] = None, max_samples: int = 0):
        p = Path(jsonl_path)
        assert p.exists(), f"Missing jsonl: {p}"
        self.items: List[Dict[str, Any]] = []
        with p.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                ex = json.loads(line)
                if split is not None and ex.get("split") != split:
                    continue
                self.items.append(ex)
                if max_samples and len(self.items) >= max_samples:
                    break

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        return self.items[idx]


# ---------------------------
# Collator
# ---------------------------
@dataclass
class Collator:
    family: str
    processor: Any
    tokenizer: Any
    max_length: int
    qwen_use_utils: bool = True

    def __post_init__(self):
        if self.family in ("qwen2_vl", "qwen2_5_vl") and self.qwen_use_utils:
            try:
                from qwen_vl_utils import process_vision_info  # type: ignore
                self._qwen_process_vision_info = process_vision_info
            except Exception as e:
                raise ImportError(
                    "For qwen2_vl/qwen2_5_vl, please install qwen-vl-utils: pip install qwen-vl-utils"
                ) from e
        else:
            self._qwen_process_vision_info = None

        try:
            from PIL import Image  # noqa
            self._PIL_ok = True
        except Exception:
            self._PIL_ok = False

    def _load_pil(self, path: str):
        if not self._PIL_ok:
            raise ImportError("PIL not available. Please install pillow.")
        from PIL import Image
        return Image.open(path).convert("RGB")

    def _encode_batch(self, texts: List[str], image_paths: List[str], qwen_video_inputs: Optional[List[Any]] = None):
        # Load images for non-Qwen families (Qwen uses process_vision_info to load).
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            return self.processor(
                text=texts,
                images=image_paths,   # for Qwen we pass PILs loaded by process_vision_info, but we overload below.
                videos=qwen_video_inputs,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
        else:
            pil_images = [self._load_pil(p) for p in image_paths]
            return self.processor(
                text=texts,
                images=pil_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # Build prompt (no answer) and full (with answer) for label masking.
        prompt_texts: List[str] = []
        full_texts: List[str] = []
        # For non-qwen families: image_paths list aligns with batch.
        img_paths: List[str] = []
        # For Qwen: we need per-sample process_vision_info outputs.
        qwen_images: List[Any] = []
        qwen_videos: List[Any] = []

        for ex in batch:
            msgs_p, imgs_p = build_messages_prompt(ex, self.family)
            msgs_f, imgs_f = build_messages_full(ex, self.family)

            if self.family in ("qwen2_vl", "qwen2_5_vl"):
                # Qwen: generate text via apply_chat_template; load vision inputs via qwen_vl_utils
                ptxt = self.processor.apply_chat_template(msgs_p, tokenize=False, add_generation_prompt=True)
                ftxt = self.processor.apply_chat_template(msgs_f, tokenize=False, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                image_inputs, video_inputs = self._qwen_process_vision_info(msgs_p)
                # Normalize "no video" case: do NOT pass empty lists to the processor.
                if video_inputs is not None and isinstance(video_inputs, (list, tuple)) and len(video_inputs) == 0:
                    video_inputs = None
                # For single-image case, image_inputs should be a list with 1 PIL image.
                # We align one image list per example by taking the first element if needed.
                # processor can accept a flat list of images aligned with text list.
                if isinstance(image_inputs, list) and len(image_inputs) == 1:
                    qwen_images.append(image_inputs[0])
                else:
                    qwen_images.append(image_inputs)
                qwen_videos.append(video_inputs if video_inputs is not None else None)
                img_paths.append(ex["image_path"])
            elif self.family == "phi3v":
                ptxt = self.tokenizer.apply_chat_template(msgs_p, tokenize=False, add_generation_prompt=True)
                ftxt = self.tokenizer.apply_chat_template(msgs_f, tokenize=False, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                img_paths.append(ex["image_path"])
            else:
                ptxt = self.processor.apply_chat_template(msgs_p, add_generation_prompt=True)
                ftxt = self.processor.apply_chat_template(msgs_f, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                img_paths.append(ex["image_path"])

        # Encode (full)
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            # IMPORTANT: do not pass `videos` when there is no video content; passing empty lists
            # can crash inside transformers video utils (IndexError).
            full_kwargs = dict(
                text=full_texts,
                images=qwen_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
            prompt_kwargs = dict(
                text=prompt_texts,
                images=qwen_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
            if any(v is not None for v in qwen_videos):
                full_kwargs["videos"] = qwen_videos
                prompt_kwargs["videos"] = qwen_videos

            full_inputs = self.processor(**full_kwargs)
            prompt_inputs = self.processor(**prompt_kwargs)
        elif self.family == "phi3v":
            # Processor signature is processor(prompt, images, ...)
            pil_images = [self._load_pil(p) for p in img_paths]
            full_inputs = self.processor(full_texts, pil_images, padding=True, truncation=True,
                                         max_length=self.max_length, return_tensors="pt")
            prompt_inputs = self.processor(prompt_texts, pil_images, padding=True, truncation=True,
                                           max_length=self.max_length, return_tensors="pt")
        else:
            pil_images = [self._load_pil(p) for p in img_paths]
            full_inputs = self.processor(text=full_texts, images=pil_images, padding=True,
                                         truncation=True, max_length=self.max_length, return_tensors="pt")
            prompt_inputs = self.processor(text=prompt_texts, images=pil_images, padding=True,
                                           truncation=True, max_length=self.max_length, return_tensors="pt")

        # Create labels: supervise only assistant response (mask prompt tokens)
        labels = full_inputs["input_ids"].clone()
        # mask padding
        labels[full_inputs["attention_mask"] == 0] = -100

        # prompt lengths
        prompt_lens = prompt_inputs["attention_mask"].sum(dim=1).tolist()
        for i, L in enumerate(prompt_lens):
            labels[i, : int(L)] = -100

        full_inputs["labels"] = labels
        return full_inputs


# ---------------------------
# Model loader
# ---------------------------
def load_model_and_processor(
    model_id: str,
    family: str,
    load_in_4bit: bool,
    bf16: bool,
    use_fast: bool,
    qwen_min_pixels: Optional[int] = None,
    qwen_max_pixels: Optional[int] = None,
) -> Tuple[torch.nn.Module, Any, Any]:
    bnb_cfg = None
    torch_dtype = torch.bfloat16 if bf16 else torch.float16

    if load_in_4bit:
        bnb_cfg = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

    if family == "llava_1_5":
        from transformers import LlavaForConditionalGeneration
        model = LlavaForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            low_cpu_mem_usage=True,
        )
        processor = safe_auto_processor_from_pretrained(model_id, use_fast=use_fast)
        tokenizer = processor.tokenizer
        return model, processor, tokenizer

    if family in ("qwen2_vl", "qwen2_5_vl", "idefics2"):
        model = AutoModelForVision2Seq.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        if family in ("qwen2_vl", "qwen2_5_vl") and (qwen_min_pixels or qwen_max_pixels):
            processor = safe_auto_processor_from_pretrained(
                model_id,
                min_pixels=qwen_min_pixels,
                max_pixels=qwen_max_pixels,
                use_fast=use_fast,
            )
        else:
            processor = safe_auto_processor_from_pretrained(model_id, use_fast=use_fast)
        tokenizer = getattr(processor, "tokenizer", AutoTokenizer.from_pretrained(model_id, trust_remote_code=True))
        return model, processor, tokenizer

    if family == "phi3v":
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        processor = safe_auto_processor_from_pretrained(model_id, trust_remote_code=True, use_fast=use_fast)
        tokenizer = processor.tokenizer
        return model, processor, tokenizer

    raise ValueError(f"Unknown family: {family}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", type=str, default=str(DEFAULT_CONFIG_PATH))
    ap.add_argument("--model_key", type=str, default="")
    ap.add_argument("--model_id", type=str)
    ap.add_argument(
        "--family",
        type=str,
        choices=["qwen2_vl", "qwen2_5_vl", "llava_1_5", "idefics2", "phi3v"],
    )
    ap.add_argument("--train_jsonl", type=str, required=True)
    ap.add_argument("--valid_jsonl", type=str, default="")
    ap.add_argument("--output_dir", type=str, required=True)

    ap.add_argument("--max_length", type=int)
    ap.add_argument("--epochs", type=float)
    ap.add_argument("--lr", type=float)
    ap.add_argument("--weight_decay", type=float)
    ap.add_argument("--warmup_ratio", type=float)

    ap.add_argument("--per_device_train_batch_size", type=int)
    ap.add_argument("--per_device_eval_batch_size", type=int)
    ap.add_argument("--grad_accum", type=int)
    ap.add_argument("--max_grad_norm", type=float)

    ap.add_argument("--bf16", action="store_true", default=None)
    ap.add_argument("--fp16", action="store_true", default=None)
    ap.add_argument("--load_in_4bit", action="store_true", default=None)
    ap.add_argument("--gradient_checkpointing", action="store_true", default=None)

    # LoRA
    ap.add_argument("--lora_r", type=int)
    ap.add_argument("--lora_alpha", type=int)
    ap.add_argument("--lora_dropout", type=float)

    # Qwen visual token budget controls (recommended for stable training)
    # Qwen2/2.5-VL expands one <image> placeholder into many "visual tokens" (default range 4-16384).
    # If your max_length is small (e.g., 512/1024), you MUST cap visual tokens via min_pixels/max_pixels.
    # You can specify either pixels (recommended by official docs) or token counts (converted using 28x28 pixels per token).
    ap.add_argument("--qwen_min_pixels", type=int, default=None, help="Min pixels for Qwen-VL resizing. 0 = auto by qwen_*_tokens.")
    ap.add_argument("--qwen_max_pixels", type=int, default=None, help="Max pixels for Qwen-VL resizing. 0 = auto by qwen_*_tokens.")
    ap.add_argument("--qwen_min_tokens", type=int)
    ap.add_argument("--qwen_max_tokens", type=int)
    ap.add_argument("--use_slow_processor", action="store_true", default=None, help="Force slow image processor (use_fast=False).")

    ap.add_argument("--seed", type=int)
    ap.add_argument("--max_train_samples", type=int)
    ap.add_argument("--max_eval_samples", type=int)

    ap.add_argument("--logging_steps", type=int)
    ap.add_argument("--save_steps", type=int)
    ap.add_argument("--eval_steps", type=int)

    args = ap.parse_args()

    cfg = {}
    if args.config and Path(args.config).exists():
        cfg = load_yaml(args.config)

    model_cfg = {}
    if args.model_key:
        model_cfg = cfg.get("models", {}).get(args.model_key, {})
    elif "model" in cfg:
        model_cfg = cfg.get("model", {})

    training_cfg = cfg.get("training", {})

    def _resolve(value, key):
        if value is not None:
            return value
        if key in training_cfg:
            return training_cfg[key]
        return DEFAULT_TRAINING[key]

    args.model_id = args.model_id or model_cfg.get("model_id")
    args.family = args.family or model_cfg.get("family")
    if not args.model_id or not args.family:
        raise ValueError("model_id/family must be provided via CLI or config.")

    args.max_length = _resolve(args.max_length, "max_length")
    args.epochs = _resolve(args.epochs, "epochs")
    args.lr = _resolve(args.lr, "lr")
    args.weight_decay = _resolve(args.weight_decay, "weight_decay")
    args.warmup_ratio = _resolve(args.warmup_ratio, "warmup_ratio")
    args.per_device_train_batch_size = _resolve(args.per_device_train_batch_size, "per_device_train_batch_size")
    args.per_device_eval_batch_size = _resolve(args.per_device_eval_batch_size, "per_device_eval_batch_size")
    args.grad_accum = _resolve(args.grad_accum, "grad_accum")
    args.max_grad_norm = _resolve(args.max_grad_norm, "max_grad_norm")
    args.bf16 = _resolve(args.bf16, "bf16")
    args.fp16 = _resolve(args.fp16, "fp16")
    args.load_in_4bit = _resolve(args.load_in_4bit, "load_in_4bit")
    args.gradient_checkpointing = _resolve(args.gradient_checkpointing, "gradient_checkpointing")
    args.lora_r = _resolve(args.lora_r, "lora_r")
    args.lora_alpha = _resolve(args.lora_alpha, "lora_alpha")
    args.lora_dropout = _resolve(args.lora_dropout, "lora_dropout")
    args.qwen_min_pixels = _resolve(args.qwen_min_pixels, "qwen_min_pixels")
    args.qwen_max_pixels = _resolve(args.qwen_max_pixels, "qwen_max_pixels")
    args.qwen_min_tokens = _resolve(args.qwen_min_tokens, "qwen_min_tokens")
    args.qwen_max_tokens = _resolve(args.qwen_max_tokens, "qwen_max_tokens")
    args.use_slow_processor = _resolve(args.use_slow_processor, "use_slow_processor")
    args.seed = _resolve(args.seed, "seed")
    args.max_train_samples = _resolve(args.max_train_samples, "max_train_samples")
    args.max_eval_samples = _resolve(args.max_eval_samples, "max_eval_samples")
    args.logging_steps = _resolve(args.logging_steps, "logging_steps")
    args.save_steps = _resolve(args.save_steps, "save_steps")
    args.eval_steps = _resolve(args.eval_steps, "eval_steps")

    set_seed(args.seed)

    bf16 = bool(args.bf16)
    if not bf16 and args.fp16:
        bf16 = False  # fp16 explicitly requested

    # ---- Qwen-VL visual token budget ----
    # Transformers will raise:
    #   ValueError: Mismatch in `image` token count between text and `input_ids` ...
    # if truncation cuts inside the expanded visual-token span.
    # To prevent this, we cap visual tokens per image by setting min_pixels/max_pixels.
    if args.family in ("qwen2_vl", "qwen2_5_vl"):
        # If user didn't explicitly pass pixel bounds, derive from token bounds.
        # Each visual token corresponds to ~28*28 pixels in the official examples.
        if args.qwen_min_pixels <= 0:
            args.qwen_min_pixels = int(args.qwen_min_tokens) * 28 * 28
        if args.qwen_max_pixels <= 0:
            args.qwen_max_pixels = int(args.qwen_max_tokens) * 28 * 28

        # Safety: ensure the image token span can fit into max_length with some room for system/prompt.
        # If user sets an extremely small max_length, warn early.
        if args.max_length and args.qwen_max_tokens >= args.max_length:
            print(
                f"[WARN] qwen_max_tokens={args.qwen_max_tokens} is >= max_length={args.max_length}. "
                f"This may cause truncation inside visual tokens. Consider lowering qwen_max_tokens "
                f"(e.g., 256) or increasing max_length (e.g., 1024+)."
            )

    # Load model \+ processor
    use_fast = not bool(args.use_slow_processor)

    model, processor, tokenizer = load_model_and_processor(
        model_id=args.model_id,
        family=args.family,
        load_in_4bit=args.load_in_4bit,
        bf16=bf16,
        use_fast=use_fast,
        qwen_min_pixels=args.qwen_min_pixels or None,
        qwen_max_pixels=args.qwen_max_pixels or None,
    )

    # Turn off cache for training
    if hasattr(model.config, "use_cache"):
        model.config.use_cache = False

    if args.gradient_checkpointing and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    # Prepare for k-bit training if needed
    if args.load_in_4bit:
        model = prepare_model_for_kbit_training(model)

    # Attach LoRA
    target_modules = infer_lora_targets(model, args.family)
    lora_cfg = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
    )
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    # Data
    train_ds = ConcreteQADataset(args.train_jsonl, split="train", max_samples=args.max_train_samples)
    eval_ds = None
    if args.valid_jsonl:
        eval_ds = ConcreteQADataset(args.valid_jsonl, split="valid", max_samples=args.max_eval_samples)

    collator = Collator(
        family=args.family,
        processor=processor,
        tokenizer=tokenizer,
        max_length=args.max_length,
    )

    # Trainer args
    fp16 = bool(args.fp16) and not bf16
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.grad_accum,
        max_grad_norm=args.max_grad_norm,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_strategy="steps" if eval_ds is not None else "no",
        eval_steps=args.eval_steps if eval_ds is not None else None,
        save_total_limit=2,
        bf16=bf16,
        fp16=fp16,
        dataloader_num_workers=2,
        remove_unused_columns=False,  # IMPORTANT for multimodal
        ddp_find_unused_parameters=False,
        report_to=[],
    )
    import inspect

    trainer_kwargs = dict(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        data_collator=collator,
    )
    sig = inspect.signature(Trainer.__init__)
    if "tokenizer" in sig.parameters:
        trainer_kwargs["tokenizer"] = tokenizer
    elif "processing_class" in sig.parameters:
        # Newer Transformers (v5+) replaced `tokenizer` with `processing_class`.
        # For multimodal models, passing the processor is appropriate.
        trainer_kwargs["processing_class"] = processor

    trainer = Trainer(**trainer_kwargs)

    trainer.train()
    trainer.save_model(args.output_dir)
    if hasattr(processor, "save_pretrained"):
        processor.save_pretrained(args.output_dir)


if __name__ == "__main__":
    main()
