#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LoRA SFT baseline runner for small/medium MLLMs on the concrete QA JSONL dataset.

Supported families (all are mainstream HF-hosted models):
  - qwen2_vl       : Qwen/Qwen2-VL-7B-Instruct
  - qwen2_5_vl     : Qwen/Qwen2.5-VL-7B-Instruct
  - llava_1_5      : llava-hf/llava-1.5-7b-hf
  - idefics2       : HuggingFaceM4/idefics2-8b
  - phi3v          : microsoft/Phi-3.5-vision-instruct

Dataset format: one json per line with fields produced by make_concrete_qa_jsonl_autosplit.py:
  image_path, system, user, assistant, task, split, meta

Multi-GPU: run with torchrun, e.g.
  torchrun --nproc_per_node=8 train_lora_vlm_baselines.py ...args...

Notes:
  - For Qwen2/2.5-VL, install qwen-vl-utils and (recommended) transformers from source.
  - This script uses PEFT LoRA. Optionally supports QLoRA (--load_in_4bit).

Author: generated by ChatGPT
"""
from __future__ import annotations

import argparse
import json
import os
import random
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import torch
from torch.utils.data import Dataset

from transformers import (
    AutoProcessor,
    AutoTokenizer,
    AutoModelForVision2Seq,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    set_seed,
)


# ---------------------------
# Processor loader (compat for transformers fast/slow processors)
# ---------------------------
def safe_auto_processor_from_pretrained(model_id: str, **kwargs):
    """Call AutoProcessor.from_pretrained with best-effort compatibility across Transformers versions."""
    try:
        return AutoProcessor.from_pretrained(model_id, **kwargs)
    except TypeError:
        # Older/newer versions may not support certain kwargs (e.g., use_fast).
        kwargs.pop("use_fast", None)
        return AutoProcessor.from_pretrained(model_id, **kwargs)

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training


# ---------------------------
# Dataset
# ---------------------------
class ConcreteQADataset(Dataset):
    def __init__(self, jsonl_path: str, split: Optional[str] = None, max_samples: int = 0):
        p = Path(jsonl_path)
        assert p.exists(), f"Missing jsonl: {p}"
        self.items: List[Dict[str, Any]] = []
        with p.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                ex = json.loads(line)
                if split is not None and ex.get("split") != split:
                    continue
                self.items.append(ex)
                if max_samples and len(self.items) >= max_samples:
                    break

    def __len__(self) -> int:
        return len(self.items)

    def __getitem__(self, idx: int) -> Dict[str, Any]:
        return self.items[idx]


# ---------------------------
# Prompt builders per family
# ---------------------------
def _base_messages(system_text: str) -> List[Dict[str, Any]]:
    msgs: List[Dict[str, Any]] = []
    if system_text:
        # Many processors accept system role. If a family ignores it, it is harmless.
        msgs.append({"role": "system", "content": [{"type": "text", "text": system_text}]})
    return msgs


def build_messages_prompt(sample: Dict[str, Any], family: str) -> Tuple[List[Dict[str, Any]], List[Any]]:
    """
    Returns:
      messages (for apply_chat_template)
      images_payload (for processor), format depends on family:
        - qwen2*: uses qwen_vl_utils.process_vision_info(messages) => images_payload is ignored here (empty)
        - others: images_payload contains PIL images (loaded later in collator)
    """
    system_text = sample.get("system", "")
    user_text = sample["user"]
    img_path = sample["image_path"]

    msgs = _base_messages(system_text)

    if family in ("qwen2_vl", "qwen2_5_vl"):
        # Qwen-VL uses its own vision parsing utils; we keep image path in the message.
        msgs.append(
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": img_path},
                    {"type": "text", "text": user_text},
                ],
            }
        )
        return msgs, []

    if family == "llava_1_5":
        # LLaVA v1.5 prompt places <image> token where {"type":"image"} appears.
        msgs.append(
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": user_text},
                    {"type": "image"},
                ],
            }
        )
        return msgs, [img_path]

    if family == "idefics2":
        msgs.append(
            {
                "role": "user",
                "content": [
                    {"type": "image"},
                    {"type": "text", "text": user_text},
                ],
            }
        )
        return msgs, [img_path]

    if family == "phi3v":
        # Phi-3.5-vision wants explicit <|image_1|> placeholders in the user content.
        placeholder = "<|image_1|>\n"
        msgs.append({"role": "user", "content": placeholder + user_text})
        return msgs, [img_path]

    raise ValueError(f"Unknown family: {family}")


def build_messages_full(sample: Dict[str, Any], family: str) -> Tuple[List[Dict[str, Any]], List[Any]]:
    msgs, imgs = build_messages_prompt(sample, family)
    # append assistant supervision
    assistant_text = sample["assistant"]
    if family == "phi3v":
        msgs.append({"role": "assistant", "content": assistant_text})
    else:
        msgs.append({"role": "assistant", "content": [{"type": "text", "text": assistant_text}]})
    return msgs, imgs


# ---------------------------
# LoRA target inference
# ---------------------------
def infer_lora_targets(model: torch.nn.Module, family: str) -> List[str]:
    """
    Collect Linear module names suitable for LoRA, based on common patterns.
    We keep this explicit because different backbones name their projections differently.
    """
    # Candidate suffixes/substrings by family.
    # These are intentionally broad; we then filter to Linear modules.
    patterns_by_family = {
        "qwen2_vl": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "qwen2_5_vl": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "llava_1_5": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "idefics2": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        # Phi often uses fused projections; keep broad.
        "phi3v": ["qkv_proj", "q_proj", "k_proj", "v_proj", "o_proj", "dense", "fc1", "fc2", "down_proj", "up_proj"],
    }
    patterns = patterns_by_family.get(family, ["q_proj", "k_proj", "v_proj", "o_proj"])

    targets: set[str] = set()
    for name, module in model.named_modules():
        if not isinstance(module, torch.nn.Linear):
            continue
        for p in patterns:
            if name.endswith(p) or (("." + p + ".") in ("." + name + ".")) or name.split(".")[-1] == p:
                targets.add(name.split(".")[-1])
                break

    # Many PEFT examples pass leaf module names (not full paths).
    # We return unique leaf names. PEFT will match all occurrences.
    targets_list = sorted(targets)

    if not targets_list:
        # Fallback: pick all leaf Linear module names in the model (dangerous but avoids silent no-op).
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Linear):
                targets.add(name.split(".")[-1])
        targets_list = sorted(targets)

    if not targets_list:
        raise RuntimeError("Could not infer LoRA targets (no Linear layers found).")

    return targets_list


# ---------------------------
# Collator
# ---------------------------
@dataclass
class Collator:
    family: str
    processor: Any
    tokenizer: Any
    max_length: int
    qwen_use_utils: bool = True

    def __post_init__(self):
        if self.family in ("qwen2_vl", "qwen2_5_vl") and self.qwen_use_utils:
            try:
                from qwen_vl_utils import process_vision_info  # type: ignore
                self._qwen_process_vision_info = process_vision_info
            except Exception as e:
                raise ImportError(
                    "For qwen2_vl/qwen2_5_vl, please install qwen-vl-utils: pip install qwen-vl-utils"
                ) from e
        else:
            self._qwen_process_vision_info = None

        try:
            from PIL import Image  # noqa
            self._PIL_ok = True
        except Exception:
            self._PIL_ok = False

    def _load_pil(self, path: str):
        if not self._PIL_ok:
            raise ImportError("PIL not available. Please install pillow.")
        from PIL import Image
        return Image.open(path).convert("RGB")

    def _encode_batch(self, texts: List[str], image_paths: List[str], qwen_video_inputs: Optional[List[Any]] = None):
        # Load images for non-Qwen families (Qwen uses process_vision_info to load).
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            return self.processor(
                text=texts,
                images=image_paths,   # for Qwen we pass PILs loaded by process_vision_info, but we overload below.
                videos=qwen_video_inputs,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
        else:
            pil_images = [self._load_pil(p) for p in image_paths]
            return self.processor(
                text=texts,
                images=pil_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )

    def __call__(self, batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
        # Build prompt (no answer) and full (with answer) for label masking.
        prompt_texts: List[str] = []
        full_texts: List[str] = []
        # For non-qwen families: image_paths list aligns with batch.
        img_paths: List[str] = []
        # For Qwen: we need per-sample process_vision_info outputs.
        qwen_images: List[Any] = []
        qwen_videos: List[Any] = []

        for ex in batch:
            msgs_p, imgs_p = build_messages_prompt(ex, self.family)
            msgs_f, imgs_f = build_messages_full(ex, self.family)

            if self.family in ("qwen2_vl", "qwen2_5_vl"):
                # Qwen: generate text via apply_chat_template; load vision inputs via qwen_vl_utils
                ptxt = self.processor.apply_chat_template(msgs_p, tokenize=False, add_generation_prompt=True)
                ftxt = self.processor.apply_chat_template(msgs_f, tokenize=False, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                image_inputs, video_inputs = self._qwen_process_vision_info(msgs_p)
                # Normalize "no video" case: do NOT pass empty lists to the processor.
                if video_inputs is not None and isinstance(video_inputs, (list, tuple)) and len(video_inputs) == 0:
                    video_inputs = None
                # For single-image case, image_inputs should be a list with 1 PIL image.
                # We align one image list per example by taking the first element if needed.
                # processor can accept a flat list of images aligned with text list.
                if isinstance(image_inputs, list) and len(image_inputs) == 1:
                    qwen_images.append(image_inputs[0])
                else:
                    qwen_images.append(image_inputs)
                qwen_videos.append(video_inputs if video_inputs is not None else None)
                img_paths.append(ex["image_path"])
            elif self.family == "phi3v":
                ptxt = self.tokenizer.apply_chat_template(msgs_p, tokenize=False, add_generation_prompt=True)
                ftxt = self.tokenizer.apply_chat_template(msgs_f, tokenize=False, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                img_paths.append(ex["image_path"])
            else:
                ptxt = self.processor.apply_chat_template(msgs_p, add_generation_prompt=True)
                ftxt = self.processor.apply_chat_template(msgs_f, add_generation_prompt=False)
                prompt_texts.append(ptxt)
                full_texts.append(ftxt)
                img_paths.append(ex["image_path"])

        # Encode (full)
        if self.family in ("qwen2_vl", "qwen2_5_vl"):
            # IMPORTANT: do not pass `videos` when there is no video content; passing empty lists
            # can crash inside transformers video utils (IndexError).
            full_kwargs = dict(
                text=full_texts,
                images=qwen_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
            prompt_kwargs = dict(
                text=prompt_texts,
                images=qwen_images,
                padding=True,
                truncation=True,
                max_length=self.max_length,
                return_tensors="pt",
            )
            if any(v is not None for v in qwen_videos):
                full_kwargs["videos"] = qwen_videos
                prompt_kwargs["videos"] = qwen_videos

            full_inputs = self.processor(**full_kwargs)
            prompt_inputs = self.processor(**prompt_kwargs)
        elif self.family == "phi3v":
            # Processor signature is processor(prompt, images, ...)
            pil_images = [self._load_pil(p) for p in img_paths]
            full_inputs = self.processor(full_texts, pil_images, padding=True, truncation=True,
                                         max_length=self.max_length, return_tensors="pt")
            prompt_inputs = self.processor(prompt_texts, pil_images, padding=True, truncation=True,
                                           max_length=self.max_length, return_tensors="pt")
        else:
            pil_images = [self._load_pil(p) for p in img_paths]
            full_inputs = self.processor(text=full_texts, images=pil_images, padding=True,
                                         truncation=True, max_length=self.max_length, return_tensors="pt")
            prompt_inputs = self.processor(text=prompt_texts, images=pil_images, padding=True,
                                           truncation=True, max_length=self.max_length, return_tensors="pt")

        # Create labels: supervise only assistant response (mask prompt tokens)
        labels = full_inputs["input_ids"].clone()
        # mask padding
        labels[full_inputs["attention_mask"] == 0] = -100

        # prompt lengths
        prompt_lens = prompt_inputs["attention_mask"].sum(dim=1).tolist()
        for i, L in enumerate(prompt_lens):
            labels[i, : int(L)] = -100

        full_inputs["labels"] = labels
        return full_inputs


# ---------------------------
# Model loader
# ---------------------------
def load_model_and_processor(
    model_id: str,
    family: str,
    load_in_4bit: bool,
    bf16: bool,
    use_fast: bool,
    qwen_min_pixels: Optional[int] = None,
    qwen_max_pixels: Optional[int] = None,
) -> Tuple[torch.nn.Module, Any, Any]:
    bnb_cfg = None
    torch_dtype = torch.bfloat16 if bf16 else torch.float16

    if load_in_4bit:
        bnb_cfg = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch_dtype,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )

    if family == "llava_1_5":
        from transformers import LlavaForConditionalGeneration
        model = LlavaForConditionalGeneration.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            low_cpu_mem_usage=True,
        )
        processor = safe_auto_processor_from_pretrained(model_id, use_fast=use_fast)
        tokenizer = processor.tokenizer
        return model, processor, tokenizer

    if family in ("qwen2_vl", "qwen2_5_vl", "idefics2"):
        model = AutoModelForVision2Seq.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        if family in ("qwen2_vl", "qwen2_5_vl") and (qwen_min_pixels or qwen_max_pixels):
            processor = safe_auto_processor_from_pretrained(
                model_id,
                min_pixels=qwen_min_pixels,
                max_pixels=qwen_max_pixels,
                use_fast=use_fast,
            )
        else:
            processor = safe_auto_processor_from_pretrained(model_id, use_fast=use_fast)
        tokenizer = getattr(processor, "tokenizer", AutoTokenizer.from_pretrained(model_id, trust_remote_code=True))
        return model, processor, tokenizer

    if family == "phi3v":
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch_dtype if not load_in_4bit else None,
            quantization_config=bnb_cfg,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
        )
        processor = safe_auto_processor_from_pretrained(model_id, trust_remote_code=True, use_fast=use_fast)
        tokenizer = processor.tokenizer
        return model, processor, tokenizer

    raise ValueError(f"Unknown family: {family}")


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model_id", type=str, required=True)
    ap.add_argument(
        "--family",
        type=str,
        required=True,
        choices=["qwen2_vl", "qwen2_5_vl", "llava_1_5", "idefics2", "phi3v"],
    )
    ap.add_argument("--train_jsonl", type=str, required=True)
    ap.add_argument("--valid_jsonl", type=str, default="")
    ap.add_argument("--output_dir", type=str, required=True)

    ap.add_argument("--max_length", type=int, default=512)
    ap.add_argument("--epochs", type=float, default=1.0)
    ap.add_argument("--lr", type=float, default=2e-4)
    ap.add_argument("--weight_decay", type=float, default=0.0)
    ap.add_argument("--warmup_ratio", type=float, default=0.03)

    ap.add_argument("--per_device_train_batch_size", type=int, default=1)
    ap.add_argument("--per_device_eval_batch_size", type=int, default=1)
    ap.add_argument("--grad_accum", type=int, default=8)
    ap.add_argument("--max_grad_norm", type=float, default=1.0)

    ap.add_argument("--bf16", action="store_true")
    ap.add_argument("--fp16", action="store_true")
    ap.add_argument("--load_in_4bit", action="store_true")
    ap.add_argument("--gradient_checkpointing", action="store_true")

    # LoRA
    ap.add_argument("--lora_r", type=int, default=16)
    ap.add_argument("--lora_alpha", type=int, default=32)
    ap.add_argument("--lora_dropout", type=float, default=0.05)

    # Qwen visual token budget controls (recommended for stable training)
    # Qwen2/2.5-VL expands one <image> placeholder into many "visual tokens" (default range 4-16384).
    # If your max_length is small (e.g., 512/1024), you MUST cap visual tokens via min_pixels/max_pixels.
    # You can specify either pixels (recommended by official docs) or token counts (converted using 28x28 pixels per token).
    ap.add_argument("--qwen_min_pixels", type=int, default=0, help="Min pixels for Qwen-VL resizing. 0 = auto by qwen_*_tokens.")
    ap.add_argument("--qwen_max_pixels", type=int, default=0, help="Max pixels for Qwen-VL resizing. 0 = auto by qwen_*_tokens.")
    ap.add_argument("--qwen_min_tokens", type=int, default=256, help="Min visual tokens per image for Qwen-VL (converted to pixels).")
    ap.add_argument("--qwen_max_tokens", type=int, default=256, help="Max visual tokens per image for Qwen-VL (converted to pixels).")
    ap.add_argument("--use_slow_processor", action="store_true", help="Force slow image processor (use_fast=False).")

    ap.add_argument("--seed", type=int, default=42)
    ap.add_argument("--max_train_samples", type=int, default=0)
    ap.add_argument("--max_eval_samples", type=int, default=0)

    ap.add_argument("--logging_steps", type=int, default=10)
    ap.add_argument("--save_steps", type=int, default=500)
    ap.add_argument("--eval_steps", type=int, default=500)

    args = ap.parse_args()

    set_seed(args.seed)

    bf16 = bool(args.bf16)
    if not bf16 and args.fp16:
        bf16 = False  # fp16 explicitly requested

    # ---- Qwen-VL visual token budget ----
    # Transformers will raise:
    #   ValueError: Mismatch in `image` token count between text and `input_ids` ...
    # if truncation cuts inside the expanded visual-token span.
    # To prevent this, we cap visual tokens per image by setting min_pixels/max_pixels.
    if args.family in ("qwen2_vl", "qwen2_5_vl"):
        # If user didn't explicitly pass pixel bounds, derive from token bounds.
        # Each visual token corresponds to ~28*28 pixels in the official examples.
        if args.qwen_min_pixels <= 0:
            args.qwen_min_pixels = int(args.qwen_min_tokens) * 28 * 28
        if args.qwen_max_pixels <= 0:
            args.qwen_max_pixels = int(args.qwen_max_tokens) * 28 * 28

        # Safety: ensure the image token span can fit into max_length with some room for system/prompt.
        # If user sets an extremely small max_length, warn early.
        if args.max_length and args.qwen_max_tokens >= args.max_length:
            print(
                f"[WARN] qwen_max_tokens={args.qwen_max_tokens} is >= max_length={args.max_length}. "
                f"This may cause truncation inside visual tokens. Consider lowering qwen_max_tokens "
                f"(e.g., 256) or increasing max_length (e.g., 1024+)."
            )

    # Load model \+ processor
    use_fast = not bool(args.use_slow_processor)

    model, processor, tokenizer = load_model_and_processor(
        model_id=args.model_id,
        family=args.family,
        load_in_4bit=args.load_in_4bit,
        bf16=bf16,
        use_fast=use_fast,
        qwen_min_pixels=args.qwen_min_pixels or None,
        qwen_max_pixels=args.qwen_max_pixels or None,
    )

    # Turn off cache for training
    if hasattr(model.config, "use_cache"):
        model.config.use_cache = False

    if args.gradient_checkpointing and hasattr(model, "gradient_checkpointing_enable"):
        model.gradient_checkpointing_enable()

    # Prepare for k-bit training if needed
    if args.load_in_4bit:
        model = prepare_model_for_kbit_training(model)

    # Attach LoRA
    target_modules = infer_lora_targets(model, args.family)
    lora_cfg = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type="CAUSAL_LM",
        target_modules=target_modules,
    )
    model = get_peft_model(model, lora_cfg)
    model.print_trainable_parameters()

    # Data
    train_ds = ConcreteQADataset(args.train_jsonl, split="train", max_samples=args.max_train_samples)
    eval_ds = None
    if args.valid_jsonl:
        eval_ds = ConcreteQADataset(args.valid_jsonl, split="valid", max_samples=args.max_eval_samples)

    collator = Collator(
        family=args.family,
        processor=processor,
        tokenizer=tokenizer,
        max_length=args.max_length,
    )

    # Trainer args
    fp16 = bool(args.fp16) and not bf16
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        learning_rate=args.lr,
        weight_decay=args.weight_decay,
        warmup_ratio=args.warmup_ratio,
        per_device_train_batch_size=args.per_device_train_batch_size,
        per_device_eval_batch_size=args.per_device_eval_batch_size,
        gradient_accumulation_steps=args.grad_accum,
        max_grad_norm=args.max_grad_norm,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_strategy="steps" if eval_ds is not None else "no",
        eval_steps=args.eval_steps if eval_ds is not None else None,
        save_total_limit=2,
        bf16=bf16,
        fp16=fp16,
        dataloader_num_workers=2,
        remove_unused_columns=False,  # IMPORTANT for multimodal
        ddp_find_unused_parameters=False,
        report_to=[],
    )
    import inspect

    trainer_kwargs = dict(
        model=model,
        args=training_args,
        train_dataset=train_ds,
        eval_dataset=eval_ds,
        data_collator=collator,
    )
    sig = inspect.signature(Trainer.__init__)
    if "tokenizer" in sig.parameters:
        trainer_kwargs["tokenizer"] = tokenizer
    elif "processing_class" in sig.parameters:
        # Newer Transformers (v5+) replaced `tokenizer` with `processing_class`.
        # For multimodal models, passing the processor is appropriate.
        trainer_kwargs["processing_class"] = processor

    trainer = Trainer(**trainer_kwargs)

    trainer.train()
    trainer.save_model(args.output_dir)
    if hasattr(processor, "save_pretrained"):
        processor.save_pretrained(args.output_dir)


if __name__ == "__main__":
    main()
